26-02-2024 23:45:41:[INFO]:__main__:train:150 - Init Encoder model... - [0:00:04.986207]
26-02-2024 23:45:42:[INFO]:__main__:train:166 - Init Predictor model... - [0:00:05.576792]
26-02-2024 23:45:42:[INFO]:__main__:train:185 - Init MultiBlockMaskCollator module... - [0:00:05.968472]
26-02-2024 23:45:42:[INFO]:__main__:train:211 - Init local dataset loading module... - [0:00:05.970405]
26-02-2024 23:45:43:[INFO]:__main__:train:221 - Dataloader is loaded! Total iteration per epoch: 24153 - [0:00:07.133961]
26-02-2024 23:45:43:[INFO]:utils:load_checkpoint:172 - Checkpoint from epoch {epoch} is successfully loaded! Extracting the parameters to load to individual model/variabels now... - [0:00:07.298559]
26-02-2024 23:45:43:[ERROR]:utils:load_checkpoint:193 - Error loading the model! Error(s) in loading state_dict for VisionTransformerForEncoder:
	size mismatch for patch_embed.patch_projection.weight: copying a param with shape torch.Size([512, 3, 14, 14]) from checkpoint, the shape in current model is torch.Size([2048, 3, 14, 14]).
	size mismatch for patch_embed.patch_projection.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.1.Wq_Wk.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.1.Wv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.0.feedforward_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.0.feedforward_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.0.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.0.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.0.feedforward_block.1.3.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.0.feedforward_block.1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.1.Wq_Wk.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.1.Wv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.1.feedforward_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.1.feedforward_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.1.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.1.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.1.feedforward_block.1.3.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.1.feedforward_block.1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.1.Wq_Wk.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.1.Wv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.2.feedforward_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.2.feedforward_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.2.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.2.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.2.feedforward_block.1.3.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.2.feedforward_block.1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.1.Wq_Wk.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.1.Wv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.3.feedforward_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.3.feedforward_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.3.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.3.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.3.feedforward_block.1.3.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.3.feedforward_block.1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.1.Wq_Wk.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.1.Wv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.4.feedforward_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.4.feedforward_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.4.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.4.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.4.feedforward_block.1.3.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.4.feedforward_block.1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.1.Wq_Wk.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.1.Wv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.5.feedforward_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.5.feedforward_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.5.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.5.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.5.feedforward_block.1.3.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.5.feedforward_block.1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.1.Wq_Wk.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.1.Wv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.6.feedforward_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.6.feedforward_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.6.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.6.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.6.feedforward_block.1.3.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.6.feedforward_block.1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.1.Wq_Wk.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.1.Wv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.7.feedforward_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.7.feedforward_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.7.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.7.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.7.feedforward_block.1.3.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.7.feedforward_block.1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.8.multi_head_attention_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.8.multi_head_attention_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.8.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).
	size mismatch for transformer_blocks.8.multi_head_attention_block.1.Wq_Wk.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for transformer_blocks.8.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
	size mismatch for transformer_blocks.8.multi_head_attention_block.1.Wv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.8.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).
	size mismatch for transformer_blocks.8.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.8.feedforward_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.8.feedforward_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.8.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.8.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.8.feedforward_block.1.3.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.8.feedforward_block.1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.9.multi_head_attention_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.9.multi_head_attention_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.9.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).
	size mismatch for transformer_blocks.9.multi_head_attention_block.1.Wq_Wk.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for transformer_blocks.9.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
	size mismatch for transformer_blocks.9.multi_head_attention_block.1.Wv.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.9.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).
	size mismatch for transformer_blocks.9.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.9.feedforward_block.0.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.9.feedforward_block.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.9.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.9.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for transformer_blocks.9.feedforward_block.1.3.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).
	size mismatch for transformer_blocks.9.feedforward_block.1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for final_layernorm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).
	size mismatch for final_layernorm.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]). - [0:00:07.299611]
26-02-2024 23:45:43:[INFO]:__main__:train:261 - Total data to train the SSL model: 386444 - [0:00:07.300885]
26-02-2024 23:45:43:[INFO]:__main__:train:264 - Training has started for epoch 0 - [0:00:07.300987]
