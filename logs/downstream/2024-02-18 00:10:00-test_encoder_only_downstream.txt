18-02-2024 00:10:53:[INFO]:__main__:downstream:130 - Init Encoder model... - [0:00:00.802777]
18-02-2024 00:10:54:[INFO]:__main__:downstream:146 - Init Predictor model... - [0:00:01.389326]
18-02-2024 00:10:54:[ERROR]:utils:load_checkpoint_downstream:196 - Error loading the model! Error(s) in loading state_dict for VisionTransformerForEncoder:
	Missing key(s) in state_dict: "transformer_blocks.8.multi_head_attention_block.0.weight", "transformer_blocks.8.multi_head_attention_block.0.bias", "transformer_blocks.8.multi_head_attention_block.1.Wq_Wk.weight", "transformer_blocks.8.multi_head_attention_block.1.Wq_Wk.bias", "transformer_blocks.8.multi_head_attention_block.1.Wv.weight", "transformer_blocks.8.multi_head_attention_block.1.Wv.bias", "transformer_blocks.8.multi_head_attention_block.1.W_o.weight", "transformer_blocks.8.multi_head_attention_block.1.W_o.bias", "transformer_blocks.8.feedforward_block.0.weight", "transformer_blocks.8.feedforward_block.0.bias", "transformer_blocks.8.feedforward_block.1.0.weight", "transformer_blocks.8.feedforward_block.1.0.bias", "transformer_blocks.8.feedforward_block.1.3.weight", "transformer_blocks.8.feedforward_block.1.3.bias", "transformer_blocks.9.multi_head_attention_block.0.weight", "transformer_blocks.9.multi_head_attention_block.0.bias", "transformer_blocks.9.multi_head_attention_block.1.Wq_Wk.weight", "transformer_blocks.9.multi_head_attention_block.1.Wq_Wk.bias", "transformer_blocks.9.multi_head_attention_block.1.Wv.weight", "transformer_blocks.9.multi_head_attention_block.1.Wv.bias", "transformer_blocks.9.multi_head_attention_block.1.W_o.weight", "transformer_blocks.9.multi_head_attention_block.1.W_o.bias", "transformer_blocks.9.feedforward_block.0.weight", "transformer_blocks.9.feedforward_block.0.bias", "transformer_blocks.9.feedforward_block.1.0.weight", "transformer_blocks.9.feedforward_block.1.0.bias", "transformer_blocks.9.feedforward_block.1.3.weight", "transformer_blocks.9.feedforward_block.1.3.bias". 
	size mismatch for patch_embed.patch_projection.weight: copying a param with shape torch.Size([256, 3, 14, 14]) from checkpoint, the shape in current model is torch.Size([512, 3, 14, 14]).
	size mismatch for patch_embed.patch_projection.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).
	size mismatch for transformer_blocks.0.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.0.feedforward_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.0.feedforward_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.0.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.0.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.0.feedforward_block.1.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.0.feedforward_block.1.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).
	size mismatch for transformer_blocks.1.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.1.feedforward_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.1.feedforward_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.1.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.1.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.1.feedforward_block.1.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.1.feedforward_block.1.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).
	size mismatch for transformer_blocks.2.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.2.feedforward_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.2.feedforward_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.2.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.2.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.2.feedforward_block.1.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.2.feedforward_block.1.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).
	size mismatch for transformer_blocks.3.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.3.feedforward_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.3.feedforward_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.3.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.3.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.3.feedforward_block.1.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.3.feedforward_block.1.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).
	size mismatch for transformer_blocks.4.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.4.feedforward_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.4.feedforward_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.4.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.4.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.4.feedforward_block.1.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.4.feedforward_block.1.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).
	size mismatch for transformer_blocks.5.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.5.feedforward_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.5.feedforward_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.5.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.5.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.5.feedforward_block.1.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.5.feedforward_block.1.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).
	size mismatch for transformer_blocks.6.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.6.feedforward_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.6.feedforward_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.6.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.6.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.6.feedforward_block.1.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.6.feedforward_block.1.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.1.Wq_Wk.weight: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.1.Wv.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.1.W_o.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).
	size mismatch for transformer_blocks.7.multi_head_attention_block.1.W_o.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.7.feedforward_block.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.7.feedforward_block.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.7.feedforward_block.1.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.7.feedforward_block.1.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for transformer_blocks.7.feedforward_block.1.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).
	size mismatch for transformer_blocks.7.feedforward_block.1.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for final_layernorm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).
	size mismatch for final_layernorm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]). - [0:00:01.623061]
18-02-2024 00:10:55:[INFO]:__main__:downstream:212 - Init dataset loading module... - [0:00:02.323568]
18-02-2024 00:10:55:[INFO]:__main__:downstream:237 - Training has started for epoch 0 - [0:00:02.384965]
